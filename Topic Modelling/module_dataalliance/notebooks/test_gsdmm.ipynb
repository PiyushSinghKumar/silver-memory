{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>text</th>\n",
       "      <th>withheld</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1550877882213433350</th>\n",
       "      <td>2022-07-23T16:17:55.000Z</td>\n",
       "      <td>Joe was cured from cancer, asthma, and probabl...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1550877888521523200</th>\n",
       "      <td>2022-07-23T16:17:57.000Z</td>\n",
       "      <td>When I wished for an #adventure on my trip to ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1550877860533092353</th>\n",
       "      <td>2022-07-23T16:17:50.000Z</td>\n",
       "      <td>BREAKING: health agency chief declares monkeyp...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1550877778651791370</th>\n",
       "      <td>2022-07-23T16:17:31.000Z</td>\n",
       "      <td>the # 1 symptom of the COVID-19 novel coronavi...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1550877892611014657</th>\n",
       "      <td>2022-07-23T16:17:58.000Z</td>\n",
       "      <td>COVID isn't working for them anymore so now th...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   created_at  \\\n",
       "_id                                             \n",
       "1550877882213433350  2022-07-23T16:17:55.000Z   \n",
       "1550877888521523200  2022-07-23T16:17:57.000Z   \n",
       "1550877860533092353  2022-07-23T16:17:50.000Z   \n",
       "1550877778651791370  2022-07-23T16:17:31.000Z   \n",
       "1550877892611014657  2022-07-23T16:17:58.000Z   \n",
       "\n",
       "                                                                  text  \\\n",
       "_id                                                                      \n",
       "1550877882213433350  Joe was cured from cancer, asthma, and probabl...   \n",
       "1550877888521523200  When I wished for an #adventure on my trip to ...   \n",
       "1550877860533092353  BREAKING: health agency chief declares monkeyp...   \n",
       "1550877778651791370  the # 1 symptom of the COVID-19 novel coronavi...   \n",
       "1550877892611014657  COVID isn't working for them anymore so now th...   \n",
       "\n",
       "                    withheld  \n",
       "_id                           \n",
       "1550877882213433350      NaN  \n",
       "1550877888521523200      NaN  \n",
       "1550877860533092353      NaN  \n",
       "1550877778651791370      NaN  \n",
       "1550877892611014657      NaN  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils import read_records, create_collection\n",
    "\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gsdmm import MovieGroupProcess\n",
    "from IPython.display import display\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import spacy\n",
    "\n",
    "\n",
    "collection = create_collection(\"covid-19\")\n",
    "tweets = [tweet for tweet in read_records(collection)]\n",
    "df = pd.DataFrame(tweets)\n",
    "df = df.set_index('_id')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/devilgoncalo/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "# functions for preprocessing\n",
    "\n",
    "# Punctuation removal + lowercasing + links, mentions, new lines, hashtags, etc\n",
    "def clean_tweet(pandas_series):\n",
    "    '''Returns a cleaned Pandas Series object with the following changes:\n",
    "    1- Removal of hashtags, new lines, retweet mentions, links and punctuation;\n",
    "    2- lowercasing.\n",
    "    '''\n",
    "    regex_transformations = [r'\\n','(https?:\\/\\/)?([\\da-z\\.-]+)\\.([a-z\\.]{2,6})([\\/\\w\\.-]*)',\n",
    "                             'RT', '@[A-Za-z0-9\\.-_:]+', '[,\\.!?:;/]']\n",
    "\n",
    "    for regex in regex_transformations:\n",
    "        pandas_series = pandas_series.map(lambda x: re.sub(regex, '', x))\n",
    "\n",
    "    pandas_series = pandas_series.map(lambda x: x.lower())\n",
    "    \n",
    "    return pandas_series\n",
    "\n",
    "\n",
    "# Tokenization\n",
    "def tweet_to_words(tweets):\n",
    "    for tweet in tweets:\n",
    "        yield(gensim.utils.simple_preprocess(str(tweet), deacc=True))\n",
    "        # deacc=True removes punctuation\n",
    "\n",
    "\n",
    "# Bigram and Trigram models\n",
    "def bigram_trigram(list_words):\n",
    "    # build the models\n",
    "    bigram = gensim.models.Phrases(list_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "    trigram = gensim.models.Phrases(bigram[list_words], threshold=100)\n",
    "    # Faster way to get a sentence clubbed as a trigram/bigram\n",
    "    bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "    trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])\n",
    "\n",
    "# Remove stopwords\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "# Make bigrams\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "decoding to str: need a bytes-like object, float found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/devilgoncalo/Westbrook/Documents/srh_masters/case_studies_1/dataalliance/test_gsdmm.ipynb Cell 2'\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/devilgoncalo/Westbrook/Documents/srh_masters/case_studies_1/dataalliance/test_gsdmm.ipynb#ch0000000vscode-remote?line=7'>8</a>\u001b[0m docs \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39mto_numpy()\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/devilgoncalo/Westbrook/Documents/srh_masters/case_studies_1/dataalliance/test_gsdmm.ipynb#ch0000000vscode-remote?line=9'>10</a>\u001b[0m \u001b[39m# create dictionary of all words in all documents\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/devilgoncalo/Westbrook/Documents/srh_masters/case_studies_1/dataalliance/test_gsdmm.ipynb#ch0000000vscode-remote?line=10'>11</a>\u001b[0m dictionary \u001b[39m=\u001b[39m gensim\u001b[39m.\u001b[39;49mcorpora\u001b[39m.\u001b[39;49mDictionary(docs)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/devilgoncalo/Westbrook/Documents/srh_masters/case_studies_1/dataalliance/test_gsdmm.ipynb#ch0000000vscode-remote?line=12'>13</a>\u001b[0m \u001b[39m# filter extreme cases out of dictionary\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/devilgoncalo/Westbrook/Documents/srh_masters/case_studies_1/dataalliance/test_gsdmm.ipynb#ch0000000vscode-remote?line=13'>14</a>\u001b[0m dictionary\u001b[39m.\u001b[39mfilter_extremes(no_below\u001b[39m=\u001b[39m\u001b[39m15\u001b[39m, no_above\u001b[39m=\u001b[39m\u001b[39m0.5\u001b[39m, keep_n\u001b[39m=\u001b[39m\u001b[39m100000\u001b[39m)\n",
      "File \u001b[0;32m~/Westbrook/Documents/srh_masters/case_studies_1/dataalliance/venv/lib/python3.8/site-packages/gensim/corpora/dictionary.py:78\u001b[0m, in \u001b[0;36mDictionary.__init__\u001b[0;34m(self, documents, prune_at)\u001b[0m\n\u001b[1;32m     <a href='file:///home/devilgoncalo/Westbrook/Documents/srh_masters/case_studies_1/dataalliance/venv/lib/python3.8/site-packages/gensim/corpora/dictionary.py?line=74'>75</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_nnz \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m     <a href='file:///home/devilgoncalo/Westbrook/Documents/srh_masters/case_studies_1/dataalliance/venv/lib/python3.8/site-packages/gensim/corpora/dictionary.py?line=76'>77</a>\u001b[0m \u001b[39mif\u001b[39;00m documents \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> <a href='file:///home/devilgoncalo/Westbrook/Documents/srh_masters/case_studies_1/dataalliance/venv/lib/python3.8/site-packages/gensim/corpora/dictionary.py?line=77'>78</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madd_documents(documents, prune_at\u001b[39m=\u001b[39;49mprune_at)\n\u001b[1;32m     <a href='file:///home/devilgoncalo/Westbrook/Documents/srh_masters/case_studies_1/dataalliance/venv/lib/python3.8/site-packages/gensim/corpora/dictionary.py?line=78'>79</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madd_lifecycle_event(\n\u001b[1;32m     <a href='file:///home/devilgoncalo/Westbrook/Documents/srh_masters/case_studies_1/dataalliance/venv/lib/python3.8/site-packages/gensim/corpora/dictionary.py?line=79'>80</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mcreated\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='file:///home/devilgoncalo/Westbrook/Documents/srh_masters/case_studies_1/dataalliance/venv/lib/python3.8/site-packages/gensim/corpora/dictionary.py?line=80'>81</a>\u001b[0m         msg\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbuilt \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m from \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_docs\u001b[39m}\u001b[39;00m\u001b[39m documents (total \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_pos\u001b[39m}\u001b[39;00m\u001b[39m corpus positions)\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='file:///home/devilgoncalo/Westbrook/Documents/srh_masters/case_studies_1/dataalliance/venv/lib/python3.8/site-packages/gensim/corpora/dictionary.py?line=81'>82</a>\u001b[0m     )\n",
      "File \u001b[0;32m~/Westbrook/Documents/srh_masters/case_studies_1/dataalliance/venv/lib/python3.8/site-packages/gensim/corpora/dictionary.py:204\u001b[0m, in \u001b[0;36mDictionary.add_documents\u001b[0;34m(self, documents, prune_at)\u001b[0m\n\u001b[1;32m    <a href='file:///home/devilgoncalo/Westbrook/Documents/srh_masters/case_studies_1/dataalliance/venv/lib/python3.8/site-packages/gensim/corpora/dictionary.py?line=200'>201</a>\u001b[0m         logger\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39madding document #\u001b[39m\u001b[39m%i\u001b[39;00m\u001b[39m to \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, docno, \u001b[39mself\u001b[39m)\n\u001b[1;32m    <a href='file:///home/devilgoncalo/Westbrook/Documents/srh_masters/case_studies_1/dataalliance/venv/lib/python3.8/site-packages/gensim/corpora/dictionary.py?line=202'>203</a>\u001b[0m     \u001b[39m# update Dictionary with the document\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/devilgoncalo/Westbrook/Documents/srh_masters/case_studies_1/dataalliance/venv/lib/python3.8/site-packages/gensim/corpora/dictionary.py?line=203'>204</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdoc2bow(document, allow_update\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)  \u001b[39m# ignore the result, here we only care about updating token ids\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/devilgoncalo/Westbrook/Documents/srh_masters/case_studies_1/dataalliance/venv/lib/python3.8/site-packages/gensim/corpora/dictionary.py?line=205'>206</a>\u001b[0m logger\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mbuilt \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m from \u001b[39m\u001b[39m%i\u001b[39;00m\u001b[39m documents (total \u001b[39m\u001b[39m%i\u001b[39;00m\u001b[39m corpus positions)\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mself\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_docs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_pos)\n",
      "File \u001b[0;32m~/Westbrook/Documents/srh_masters/case_studies_1/dataalliance/venv/lib/python3.8/site-packages/gensim/corpora/dictionary.py:246\u001b[0m, in \u001b[0;36mDictionary.doc2bow\u001b[0;34m(self, document, allow_update, return_missing)\u001b[0m\n\u001b[1;32m    <a href='file:///home/devilgoncalo/Westbrook/Documents/srh_masters/case_studies_1/dataalliance/venv/lib/python3.8/site-packages/gensim/corpora/dictionary.py?line=243'>244</a>\u001b[0m counter \u001b[39m=\u001b[39m defaultdict(\u001b[39mint\u001b[39m)\n\u001b[1;32m    <a href='file:///home/devilgoncalo/Westbrook/Documents/srh_masters/case_studies_1/dataalliance/venv/lib/python3.8/site-packages/gensim/corpora/dictionary.py?line=244'>245</a>\u001b[0m \u001b[39mfor\u001b[39;00m w \u001b[39min\u001b[39;00m document:\n\u001b[0;32m--> <a href='file:///home/devilgoncalo/Westbrook/Documents/srh_masters/case_studies_1/dataalliance/venv/lib/python3.8/site-packages/gensim/corpora/dictionary.py?line=245'>246</a>\u001b[0m     counter[w \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(w, \u001b[39mstr\u001b[39m) \u001b[39melse\u001b[39;00m \u001b[39mstr\u001b[39;49m(w, \u001b[39m'\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m'\u001b[39;49m)] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    <a href='file:///home/devilgoncalo/Westbrook/Documents/srh_masters/case_studies_1/dataalliance/venv/lib/python3.8/site-packages/gensim/corpora/dictionary.py?line=247'>248</a>\u001b[0m token2id \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtoken2id\n\u001b[1;32m    <a href='file:///home/devilgoncalo/Westbrook/Documents/srh_masters/case_studies_1/dataalliance/venv/lib/python3.8/site-packages/gensim/corpora/dictionary.py?line=248'>249</a>\u001b[0m \u001b[39mif\u001b[39;00m allow_update \u001b[39mor\u001b[39;00m return_missing:\n",
      "\u001b[0;31mTypeError\u001b[0m: decoding to str: need a bytes-like object, float found"
     ]
    }
   ],
   "source": [
    "# cast tweets to numpy array\n",
    "docs = df.to_numpy()\n",
    "\n",
    "# create dictionary of all words in all documents\n",
    "dictionary = gensim.corpora.Dictionary(docs)\n",
    "\n",
    "# filter extreme cases out of dictionary\n",
    "dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)\n",
    "\n",
    "# create variable containing length of dictionary/vocab\n",
    "vocab_length = len(dictionary)\n",
    "\n",
    "# create BOW dictionary\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in docs]\n",
    "\n",
    "# initialize GSDMM\n",
    "gsdmm = MovieGroupProcess(K=15, alpha=0.1, beta=0.3, n_iters=15)\n",
    "\n",
    "# fit GSDMM model\n",
    "y = gsdmm.fit(docs, vocab_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b2d883c8bd3e6e49837812bcb97c0e5ebdc696ca38fddee559b3f0208fef7d8e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
